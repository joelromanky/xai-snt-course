{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a28e81e",
   "metadata": {},
   "source": [
    "# Explainability Lab\n",
    "**Date:** 2025-11-28\n",
    "\n",
    "**What this notebook covers**\n",
    "\n",
    "\n",
    "**Part 0** (common for each part):\n",
    "  - Setup & Imports\n",
    "  - Load & preprocess the UCI Adult dataset.\n",
    "  - Train a Random Forest Classifier.\n",
    "  - Train a PyTorch MLP classifier.\n",
    "\n",
    "**Part 1** (Perturbation explanations (LIME/SHAP))\n",
    "\n",
    "- Generate and visualize feature attributions with **SHAP** and **Lime**.\n",
    "\n",
    "**Part 2** (Gradient-based explanations using Captum/Zennit)\n",
    "\n",
    "- Generate and visualize feature attributions using **Captum** gradient-based methods:\n",
    "  - Saliency\n",
    "  - SmoothGrad (NoiseTunnel)\n",
    "  - InputxGradients\n",
    "  - Integrated Gradients.\n",
    "- Generate and visualize feature attributions using **Zennit** LRP methods:\n",
    "  - LRP (Layer-wise Relevance Propagation).\n",
    "- Evaluate every XAI methods with **Quantus/Captum** metrics.\n",
    "\n",
    "**Part 3** (Counterfactual explanations)\n",
    "- Generate and evaluate counterfactual explanations for tabular data using **Dice**.\n",
    "\n",
    "**Part 4** (Attacks on XAI)\n",
    "- Implement an attack to fool LIME/SHAP explanations.\n",
    "\n",
    "**Part 5** (Explanations on LLMs)\n",
    "- Generate and visualize feature attributions for LLM using **Captum** and **LXT**.\n",
    "\n",
    "\n",
    "\n",
    "> Notes:\n",
    "- This notebook expects an environment with internet to fetch the dataset.\n",
    "- For part 5, it expects to have GPU on your machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499350f6",
   "metadata": {},
   "source": [
    "# Part 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bb501",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from captum.attr import (\n",
    "    Saliency,\n",
    "    IntegratedGradients,\n",
    "    NoiseTunnel,\n",
    "    InputXGradient,\n",
    ")\n",
    "\n",
    "from zennit.attribution import Gradient\n",
    "\n",
    "import quantus\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # safe even if no GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29ea21",
   "metadata": {},
   "source": [
    "## Load & Preprocess the Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68680f3",
   "metadata": {},
   "source": [
    "In this section we:\n",
    "1. Download the UCI Adult dataset via OpenML.\n",
    "2. Clean basic missing values.\n",
    "3. Separate target vs features.\n",
    "4. Apply preprocessing (scaling numeric features, encoding categorical ones).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Adult from OpenML\n",
    "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "df = adult.frame.copy()\n",
    "\n",
    "# Replace '?' with NaN and drop rows with missing (simple but aggressive approach).\n",
    "# In practice you may want something more subtle (imputation, etc.).\n",
    "df = df.replace('?', np.nan).dropna()\n",
    "\n",
    "# Target is 'class': '>50K' or '<=50K' —> convert to 0/1\n",
    "df['class'] = (df['class'] == '>50K').astype(int)\n",
    "\n",
    "# Identify categorical vs numeric columns\n",
    "target_col = 'class'\n",
    "X_df = df.drop(columns=[target_col])\n",
    "y = df[target_col].values\n",
    "\n",
    "# Identify categorical vs numeric columns (based on dtype).\n",
    "cat_cols = X_df.select_dtypes(include=['category','object']).columns.tolist()\n",
    "num_cols = [c for c in X_df.columns if c not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8057f",
   "metadata": {},
   "source": [
    "Build preprocessing pipeline:\n",
    "- `StandardScaler` for numeric columns (zero mean, unit variance).\n",
    "- `OrdinalEncoder` for categorical columns.\n",
    "\n",
    "We **could** use `OneHotEncoder`, but that can create many sparse features.\n",
    "Here we use OrdinalEncoder to keep the feature space compact, which simplifies some XAI methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c302f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
    "        ('cat', OrdinalEncoder(), cat_cols), # We use here OrdinalEncoder to limit the number of features\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocess.fit_transform(X_df)\n",
    "\n",
    "# Store the feature names after preprocessing for later interpretation.\n",
    "feature_names_num = num_cols\n",
    "feature_names_cat = list(preprocess.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "feature_names_all = feature_names_num + feature_names_cat\n",
    "\n",
    "# Split into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57a660",
   "metadata": {},
   "source": [
    "We now convert the NumPy arrays to PyTorch tensors and build DataLoaders for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ff634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the torch tensors\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1,1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "if (y_train_t.ndim == 1) or (y_train_t.shape[1] == 1):\n",
    "    y_train_t = torch.column_stack((1 - y_train_t, y_train_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap tensors into TensorDataset and DataLoader for batching.\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553b89e",
   "metadata": {},
   "source": [
    "## Scikit Learn Random Forest Model & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2bab0",
   "metadata": {},
   "source": [
    "We first train a classical `RandomForestClassifier` tree ensemble on the preprocessed features. This will be our baseline model for **SHAP** and **LIME** explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities and hard predictions on the test set.\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "y_pred = rf.predict(X_test)\n",
    "#y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_test, y_prob), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e86230",
   "metadata": {},
   "source": [
    "## PyTorch MLP Model & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d70e98",
   "metadata": {},
   "source": [
    "We now build a simple fully-connected neural network for the same task,\n",
    "which we will then analyze using **SHAP, LIME, Captum** and **Zennit**.\n",
    "\n",
    ">Note: this is not a highly tuned architecture; the goal is to have\n",
    "a reasonably accurate yet simple model, not to win Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b549a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple fully-connected MLP for tabular classification.\n",
    "\n",
    "    The network consists of:\n",
    "      - An input layer projecting from `input_dim` to `hidden_dim`.\n",
    "      - `n_layers - 1` hidden layers of size `hidden_dim` with ReLU activations.\n",
    "      - A final linear layer projecting to `output_dim` (number of classes).\n",
    "      - A softmax over the output to obtain class probabilities.\n",
    "\n",
    "    Args:\n",
    "        n_layers (int): Total number of linear layers (including output).\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dim (int): Number of units in each hidden layer.\n",
    "        output_dim (int): Number of output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()     \n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input Layer (= first hidden layer)\n",
    "        layers += [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "\n",
    "        # Hidden Layers (number specified by n_layers)\n",
    "        for _ in range(n_layers -1):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU() ]\n",
    "\n",
    "        # Output Layer\n",
    "        layers += [nn.Linear(hidden_dim, output_dim)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, output_dim) with class probabilities.\n",
    "        \"\"\"\n",
    "        x = self.network(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5444903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Utility class to keep track of running averages (e.g. loss during training).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Current value, average, cumulative sum, and count.\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the class attributes.\n",
    "        \"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n_count=1):\n",
    "        \"\"\"\n",
    "        Update the values.\n",
    "\n",
    "        Args:\n",
    "            val (float): Current value.\n",
    "            n_count (int, optional): Number of current value. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * n_count\n",
    "        self.count += n_count\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "          train_loader: DataLoader,\n",
    "          num_epochs: int,\n",
    "          criterion: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          device: torch.device\n",
    "          ) -> None:\n",
    "    \"\"\"\n",
    "    Simple training loop for a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: Model to train.\n",
    "        train_loader: DataLoader yielding (inputs, targets) batches.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer (e.g. Adam).\n",
    "        device: Device on which to run the computation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            logits = model(x_batch)\n",
    "            # print(logits.shape, y_batch.shape)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_meter.update(loss.item())\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | loss={loss_meter.avg:.3f}\")\n",
    "\n",
    "\n",
    "def predict(model: nn.Module,\n",
    "            test_loader: DataLoader,\n",
    "            device: torch.device\n",
    "            ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute class predictions for all samples in a DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        test_loader: DataLoader for the evaluation set.\n",
    "        device: Device on which to run inference.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of predicted class indices of shape (N,).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # y_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader:\n",
    "            preds = model(batch_X.to(device))\n",
    "            # preds = torch.sigmoid(logits)\n",
    "            predictions.append(preds.detach().cpu().numpy())\n",
    "\n",
    "            # y_true.append(batch_y.cpu().numpy())\n",
    "\n",
    "    probas = np.concatenate(predictions)\n",
    "\n",
    "    # If binary task returns only probability for the true class, adapt it to return (N x 2)\n",
    "    if probas.shape[1] == 1:\n",
    "        probas = np.concatenate((1 - probas, probas), 1)\n",
    "\n",
    "    predictions = np.argmax(probas, axis=1)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c6df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model and training hyperparameters.\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_layers = 4\n",
    "input_dim = X_train_t.shape[1]\n",
    "hidden_dim = 47\n",
    "output_dim = 2 # number of classes\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "\n",
    "model = MLPModel(n_layers=n_layers,\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim\n",
    "                ).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65273a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network.\n",
    "train(model,\n",
    "      train_loader=train_loader,\n",
    "      num_epochs=num_epochs,\n",
    "      criterion=criterion,\n",
    "      optimizer=optimizer,\n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set.\n",
    "pred = predict(model, test_loader, device)\n",
    "\n",
    "acc = (pred==y_test).mean()\n",
    "print(f\"Accuracy on the test set : {acc*100 :2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted labels to a tensor (used later as \"target\" for some XAI methods).\n",
    "y_pred_t = torch.from_numpy(pred).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613a003",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209b56b",
   "metadata": {},
   "source": [
    "## SHAP/LIME explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64588008",
   "metadata": {},
   "source": [
    "### SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16df35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# Explain the model's predictions using SHAP.\n",
    "# For tree-based models, SHAP can use highly optimized algorithms.\n",
    "explainer = shap.Explainer(rf, feature_names=feature_names_all)\n",
    "shap_values = explainer(X_test[:100]) # Explain first 100 test instances for time constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot\n",
    "shap.plots.waterfall(shap_values[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot\n",
    "shap.plots.force(shap_values[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ac2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all the explanations\n",
    "shap.plots.force(shap_values[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ffbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot\n",
    "shap.plots.beeswarm(shap_values[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a946f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "shap.plots.bar(shap_values[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d51b83",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b0c56",
   "metadata": {},
   "source": [
    "We now apply LIME to the same Random Forest model.\n",
    "LIME learns a local surrogate model around one specific input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbd30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Note: here we pass `X_train` as the background data in the preprocessed space.\n",
    "explainer_lime = LimeTabularExplainer(\n",
    "    training_data=X_train,\n",
    "    feature_names=feature_names_all,\n",
    "    class_names=['<=50K','>50K'],\n",
    "    categorical_features=feature_names_cat,\n",
    "    discretize_continuous=True,\n",
    "    random_state=SEED)\n",
    "\n",
    "idx = 0 ## index of the test instance to explain\n",
    "x_raw = X_test[idx]\n",
    "exp = explainer_lime.explain_instance(\n",
    "    data_row=np.array(x_raw),\n",
    "    predict_fn=rf.predict_proba,\n",
    "    num_features=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e764558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim for LIME + modern IPython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "html = exp.as_html()\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eece55c",
   "metadata": {},
   "source": [
    ">TODO\n",
    "\n",
    "Restart these sections and generate SHAP and LIME explanations using either the Pytorch MLP (of the next sections) or a XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e115865",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da412d",
   "metadata": {},
   "source": [
    "## Captum Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78521331",
   "metadata": {},
   "source": [
    "We now use [Captum](https://captum.ai/) to compute gradient-based feature attributions for the MLP.\n",
    "We will:\n",
    "- Implement helper visualization utilities (global bar plots + beeswarm-style scatter).\n",
    "- Compute **Saliency**, **SmoothGrad**, **Input×Gradient**, and **Integrated Gradients**.\n",
    "- Compare them qualitatively and quantitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed0bd0",
   "metadata": {},
   "source": [
    "### Visualization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to print importances and visualize distribution\n",
    "def visualize_importances(feature_names: List[str],\n",
    "                        importances: np.ndarray,\n",
    "                        title: str = \"Average Feature Importances\",\n",
    "                        plot: bool = True,\n",
    "                        axis_title: str = \"Features\"\n",
    "                        ) -> None:\n",
    "    \"\"\"\n",
    "    Print and optionally plot average feature importances.\n",
    "\n",
    "    Args:\n",
    "        feature_names: List of feature names (length = n_features).\n",
    "        importances: Array of importances of shape (n_features,).\n",
    "        title: Title for the plot/printout.\n",
    "        plot: If True, displays a bar plot using Matplotlib.\n",
    "        axis_title: Label for the x-axis.\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.4f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(x_pos, importances, align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True, rotation=45)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "def beeswarm_attributions(\n",
    "        attrs: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        feature_names: List[str],\n",
    "        max_display: int = 20,\n",
    "        color_by: str = \"feature\",\n",
    "        cmap=None,\n",
    "        jitter: float = 0.25,\n",
    "        dot_size: int = 8,\n",
    "        title: str = \"Beeswarm of Attributions\",\n",
    "        xlabel: str = \"Attribution (signed)\",\n",
    "):\n",
    "    \"\"\"\n",
    "    SHAP-style beeswarm plot for tabular attributions.\n",
    "\n",
    "    Args:\n",
    "        attrs: Array of shape (n_samples, n_features) with signed attributions.\n",
    "        X: Original input values (same shape as attrs).\n",
    "        feature_names: List of feature names.\n",
    "        max_display: Maximum number of features to display (sorted by mean |attr|).\n",
    "        color_by: Whether to color points by \"feature\" values, \"attr\" values, or None.\n",
    "        cmap: Matplotlib colormap name or object.\n",
    "        jitter: Vertical jitter scale to avoid overlap.\n",
    "        dot_size: Marker size.\n",
    "        title: Plot title.\n",
    "        xlabel: Label for the x-axis.\n",
    "    \"\"\"\n",
    "    attrs = np.asarray(attrs)\n",
    "    X = np.asarray(X)\n",
    "    assert attrs.shape == X.shape, \"attrs and X must have same shape [n_samples, n_features]\"\n",
    "    n_samples, n_features = attrs.shape\n",
    "    feature_names = list(feature_names)\n",
    "\n",
    "    # Rank features by mean absolute attribution\n",
    "    mean_abs = np.mean(np.abs(attrs), axis=0)\n",
    "    order = np.argsort(-mean_abs)[:max_display]\n",
    "    attrs_sub = attrs[:, order]\n",
    "    X_sub = X[:, order]\n",
    "    names_sub = [feature_names[i] for i in order]\n",
    "\n",
    "    # Prepare figure\n",
    "    plt.figure(figsize=(10, 0.4 * len(names_sub) + 2))\n",
    "    y_base = np.arange(len(names_sub))  # one row per feature (top is most important)\n",
    "    y_plot_positions = []\n",
    "\n",
    "    # Normalize color reference per-feature (like SHAP)\n",
    "    def normalize_col(v):\n",
    "        v = v.astype(float)\n",
    "        vmin, vmax = np.nanmin(v), np.nanmax(v)\n",
    "        if vmax == vmin:\n",
    "            return np.zeros_like(v)  # flat color if constant\n",
    "        return (v - vmin) / (vmax - vmin)\n",
    "\n",
    "    for j, (a_col, x_col) in enumerate(zip(attrs_sub.T, X_sub.T)):\n",
    "        # Jitter to avoid overplotting; more points near 0 should stack, not overlap\n",
    "        # Use rank-based spread to get a “swarm” feel\n",
    "        # We place points around y = (len(names_sub)-1 - j) so most important is at top\n",
    "        y0 = (len(names_sub) - 1 - j)\n",
    "        # Create a small symmetric jitter using ranks of attribution values\n",
    "        ranks = a_col.argsort().argsort()  # 0..n-1 ranks\n",
    "        # Center ranks around 0 and scale\n",
    "        jitter_offsets = (ranks - np.median(ranks)) / (np.max(ranks) + 1e-9)\n",
    "        y_vals = y0 + jitter * jitter_offsets\n",
    "        y_plot_positions.append(y0)\n",
    "\n",
    "        # Colors\n",
    "        if color_by == \"feature\":\n",
    "            cvals = normalize_col(x_col)\n",
    "            sc = plt.scatter(a_col, y_vals, s=dot_size, c=cvals, cmap=cmap, alpha=0.8, edgecolors='none')\n",
    "        elif color_by == \"attr\":\n",
    "            cvals = normalize_col(a_col)\n",
    "            sc = plt.scatter(a_col, y_vals, s=dot_size, c=cvals, cmap=cmap, alpha=0.8, edgecolors='none')\n",
    "        else:\n",
    "            sc = plt.scatter(a_col, y_vals, s=dot_size, alpha=0.8, edgecolors='none')\n",
    "\n",
    "    # Axes & labels\n",
    "    plt.yticks(np.arange(len(names_sub)), names_sub)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='x', linestyle=':', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Optional colorbar\n",
    "    if color_by in (\"feature\", \"attr\") and cmap is not None:\n",
    "        cbar = plt.colorbar(sc, pad=0.01)\n",
    "        cbar.set_label(\"Feature value\" if color_by == \"feature\" else \"Attribution (normalized)\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654ac62",
   "metadata": {},
   "source": [
    "### Captum/Quantus metrics utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b9ede",
   "metadata": {},
   "source": [
    "We use **Quantus** library which is an explainable AI toolit to evaluate neural network explanations.   \n",
    "A lot of XAI metrics including\n",
    "- Faithfulness metrics\n",
    "- Robustness metrics\n",
    "- Localisation and so on.\n",
    "\n",
    "Check the [Github repository](https://github.com/understandable-machine-intelligence-lab/Quantus/tree/main) for more details.\n",
    "\n",
    "In this lab, we will use two (02) faithfulness metrics: `Sufficiency` and `FaithfulnessEstimate` and three (03) robustness metrics: `RIS`, `ROS` and `Consistency`.\n",
    "\n",
    ">Note:\n",
    "Most of the metrics implemented in Quantus are compatible with **image datasets** and may not be applicable to tabular data. Check the documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to see available metrics in Captum\n",
    "quantus.AVAILABLE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f55c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantus_metrics_multi(model: nn.Module,\n",
    "                              xai_wrappers: List[Callable],\n",
    "                              xai_names: List[str],\n",
    "                              x_data: np.ndarray,\n",
    "                              y_data: np.ndarray,\n",
    "                              device: torch.device,\n",
    "                              verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluates a model's XAI methods against a set of Quantus metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model (nn.Module).\n",
    "        xai_wrappers: A list of Callable XAI explanation functions/wrappers.\n",
    "        xai_names: A list of strings corresponding to the names of the XAI methods.\n",
    "                   Must be the same length as xai_wrappers.\n",
    "        x_data: Input data batch (np.ndarray).\n",
    "        y_data: Target label batch (np.ndarray).\n",
    "        verbose: If True, prints evaluation details.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the evaluation results from quantus.evaluate.\n",
    "    \"\"\"\n",
    "    if len(xai_wrappers) != len(xai_names):\n",
    "        raise ValueError(\"The length of 'xai_wrappers' must match the length of 'xai_names'.\")\n",
    "\n",
    "    # Ensure model is in evaluation mode and on the correct device\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Define Quantus metrics\n",
    "    metrics = {\n",
    "        \"RIS\": quantus.RelativeInputStability(nr_samples=5),\n",
    "        \"ROS\": quantus.RelativeOutputStability(nr_samples=5),\n",
    "        \"Consistency\": quantus.Consistency(discretise_func=quantus.functions.discretise_func.top_n_sign,\n",
    "                                           return_aggregate=False),\n",
    "        \"Sufficiency\": quantus.Sufficiency(threshold=0.6,\n",
    "                                           return_aggregate=False),\n",
    "        \"Faithfulness\": quantus.FaithfulnessEstimate(abs=False,\n",
    "                                                     normalise=False,\n",
    "                                                     features_in_step=1,  \n",
    "                                                     perturb_baseline=\"mean\"),\n",
    "    }\n",
    "    \n",
    "    # Construct the XAI methods dictionary\n",
    "    # This uses a dictionary comprehension to map names to functions\n",
    "    xai_methods = dict(zip(xai_names, xai_wrappers))\n",
    "\n",
    "    # Quantus config \n",
    "    # explain_func_kwargs → required (even if empty)\n",
    "    explain_func_kwargs = {}\n",
    "\n",
    "    # call_kwargs for the metric\n",
    "    call_kwargs = {\"run\": {\"device\": device}}\n",
    "\n",
    "    # Evaluate\n",
    "    results = quantus.evaluate(\n",
    "        metrics=metrics,\n",
    "        xai_methods=xai_methods,\n",
    "        model=model,\n",
    "        x_batch=x_data,\n",
    "        y_batch=y_data,\n",
    "        explain_func_kwargs=explain_func_kwargs,\n",
    "        call_kwargs=call_kwargs,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906200e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_results(results: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts metric data from the first set of results, cleans it by\n",
    "    removing NaN values, and returns the cleaned data as a dictionary\n",
    "    of NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        results: A dictionary where the values are metric dictionaries\n",
    "                 (e.g., {'run_1': {'metric_a': [1, 2, np.nan], ...}}).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping metric names to their cleaned 1D NumPy arrays.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return {}\n",
    "\n",
    "    # Get the metric dictionary from the first result set\n",
    "    first_metrics_data = next(iter(results.values()))\n",
    "\n",
    "    # Process each metric in the first set\n",
    "    cleaned_metrics = {}\n",
    "    for metric_name, values_list in first_metrics_data.items():\n",
    "        metric_array = np.array(values_list, dtype=np.float64)\n",
    "\n",
    "        # Remove NaN values using Boolean indexing\n",
    "        #cleaned_array = metric_array[~np.isnan(metric_array)]\n",
    "        \n",
    "        #cleaned_metrics[metric_name] = cleaned_array\n",
    "        cleaned_metrics[metric_name] = metric_array\n",
    "    \n",
    "    return cleaned_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225fb7a",
   "metadata": {},
   "source": [
    "We also include `Infidelity` and `Sensitivity` faithfulness metrics from **Captum** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr._utils.attribution import GradientAttribution\n",
    "from captum.metrics import sensitivity_max, infidelity\n",
    "\n",
    "def get_captum_metrics(captum_attribution: GradientAttribution,\n",
    "                       inputs: torch.Tensor,\n",
    "                       target: torch.Tensor,\n",
    "                       attribution: torch.Tensor,\n",
    "                       device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Get Captum Sensitivity and Infidelity metrics.\n",
    "\n",
    "    Args:\n",
    "        captum_attribution (GradientAttribution): Captum attribution object.\n",
    "        inputs (torch.Tensor): Data features from which the attribution are computed.\n",
    "        target (torch.Tensor): Target predictions.\n",
    "        attribution (torch.Tensor): Captum attributions.\n",
    "        device (torch.device): Torch device.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Sensitivity and Infidelity scores as nmpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    def perturb_fn(inputs):\n",
    "        # Add small Gaussian noise and return noisy inputs and corresponding baseline.\n",
    "        noise = torch.tensor(np.random.normal(0, 0.003, inputs.shape)).to(inputs.device).float()\n",
    "        return noise, inputs - noise\n",
    "    \n",
    "    sens = sensitivity_max(captum_attribution.attribute,\n",
    "                           inputs.to(device),\n",
    "                           target=target.to(device)\n",
    "                           ).detach().cpu().numpy()\n",
    "\n",
    "    infid = infidelity(model, perturb_fn,\n",
    "                       inputs=inputs.to(device),\n",
    "                       attributions=attribution.to(device),\n",
    "                       target=target.to(device)\n",
    "                       ).detach().cpu().numpy()\n",
    "    return sens, infid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0111afc",
   "metadata": {},
   "source": [
    "### Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce2eda",
   "metadata": {},
   "source": [
    "A baseline approach for computing input attribution. It returns the gradients with respect to inputs. If abs is set to True, which is the default, the absolute value of the gradients is returned.\n",
    "\n",
    "More details about the approach can be found in the following paper:\n",
    "https://arxiv.org/abs/1312.6034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42248c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the salience explainer with the forward function of the model.\n",
    "xai_saliency = Saliency(model)\n",
    "\n",
    "attr = xai_saliency.attribute(X_test_t.requires_grad_(True).to(device), # inputs for which explanations are computed \n",
    "                              target=y_pred_t, # Target\n",
    "                              abs=False \n",
    "                              )\n",
    "attr = attr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the attributions feature-wise (max absolute value = 1) to make\n",
    "# them easier to compare across features. (because Captum just return the gradients)\n",
    "\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d09b9a",
   "metadata": {},
   "source": [
    "#### Generate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from normalized saliency.\n",
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm-style visualization of saliency attributions.\n",
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148dfee",
   "metadata": {},
   "source": [
    "#### Compute explanations metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_wrapper(model: nn.Module,\n",
    "                    inputs: np.ndarray,\n",
    "                    targets: np.ndarray,\n",
    "                    **kwargs\n",
    "                    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper around Captum's Saliency so it matches the interface expected by Quantus.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model (not used explicitly, but required by Quantus).\n",
    "        inputs: Input batch as NumPy array.\n",
    "        targets: Class indices as NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        Attributions as NumPy array of same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    # Compute attributions (Captum expects tensor inputs)\n",
    "    attributions = xai_saliency.attribute(x_t, target=y_t, abs=False)\n",
    "    return attributions.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e64aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[saliency_wrapper],\n",
    "                                            xai_names=[\"saliency\"],\n",
    "                                            x_data=X_test,\n",
    "                                            y_data=pred,\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = get_captum_metrics(captum_attribution=xai_saliency,\n",
    "                                    inputs=X_test_t,\n",
    "                                    target=y_pred_t,\n",
    "                                    attribution=torch.from_numpy(attr),\n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics_results(quantus_metrics)\n",
    "all_metrics[\"Sensitivity\"] = captum_metrics[0]\n",
    "all_metrics[\"Infidelity\"] = captum_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e81438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware: RIS/ROS can produce NaN values for some samples.\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e3929",
   "metadata": {},
   "source": [
    "### SmoothGrad (NoiseTunnel over Saliency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade1a8a",
   "metadata": {},
   "source": [
    "To implement **SmoothGrad**, we add gaussian noise using `NoiseTunnel` to each input in the batch `nt_samples` times and applies the `Saliency` attribution algorithm to each of the samples. The attributions of the samples are combined based on the given noise tunnel type (nt_type). Here we use `nt_type=\"smoothgrad\"`, and the mean of the sampled attributions is returned. Start with lower values of `nt_samples=10` to reduce computational time.\n",
    "\n",
    "More details can be find in this [paper](https://arxiv.org/abs/1706.03825).\n",
    "\n",
    "See [Captum documentation](https://captum.ai/api/noise_tunnel.html) to help for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cfef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate explanations for SmoothGrad\n",
    "\n",
    "#TODO\n",
    "\n",
    "xai_sg = ...\n",
    "attr = ...\n",
    "\n",
    "# Normalize the attributions.\n",
    "\n",
    "attr_norm = attr ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9638623",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# feature beeswarm visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852ae90",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68dc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad_wrapper(model: nn.Module,\n",
    "                     inputs: np.ndarray,\n",
    "                     targets:np.ndarray,\n",
    "                     **kwargs\n",
    "                     ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper around SmoothGrad (NoiseTunnel over Saliency) for Quantus.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO\n",
    "    raise NotImplementedError(\"Implement the smoothgrad wrapper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44495d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa74cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55301824",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743ce2",
   "metadata": {},
   "source": [
    "### Input x Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20dc93",
   "metadata": {},
   "source": [
    "A baseline approach for computing the attribution. It multiplies input with the gradient with respect to input. \n",
    "\n",
    "More details in the [paper](https://arxiv.org/abs/1605.01713).\n",
    "\n",
    "See [Captum documentation](https://captum.ai/api/input_x_gradient.html) to help for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate explanations for InputxGradient\n",
    "\n",
    "xai_ig = ...\n",
    "\n",
    "attr = ...\n",
    "\n",
    "# Normalize the attributions.\n",
    "\n",
    "attr_norm = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97626a7",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# feature beeswarm visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb1539",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc76022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_grad_wrapper(model, inputs, targets, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper around InputxGradient for Quantus.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    raise NotImplementedError(\"Implement the inputxgradient wrapper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02853e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b26291",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123c7c4",
   "metadata": {},
   "source": [
    "### Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b50da",
   "metadata": {},
   "source": [
    "Integrated Gradients is an axiomatic model interpretability algorithm that assigns an importance score to each input feature by approximating the integral of gradients of the model’s output with respect to the inputs along the path (straight line) from given baselines / references to inputs.\n",
    "\n",
    "Baselines can be provided. We could give a zero tensor with the same shape as the input. Here, we provide the mean vector of the train set.\n",
    "\n",
    "More details regarding the integrated gradients method can be found in the original [paper](https://arxiv.org/abs/1703.01365)\n",
    "\n",
    "See [Captum documentation](https://captum.ai/api/integrated_gradients.html) to help for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_int_grad = IntegratedGradients(model)\n",
    "baseline = X_train_t.mean(dim=0) # Take on input (as the mean of the training set)\n",
    "baselines = baseline.repeat(X_test_t.shape[0], 1).to(device) # Cast to the shape of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = ...\n",
    "\n",
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "\n",
    "attr_norm = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38257b1",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# feature beeswarm visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8f345",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intgrad_wrapper(model, inputs, targets, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper around IntegratedGradients for Quantus.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    raise NotImplementedError(\"Implement the IntegratedGradients wrapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaef2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31056723",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17555a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada00c5",
   "metadata": {},
   "source": [
    "### LRP (Layer-wise Relevance Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d605c5",
   "metadata": {},
   "source": [
    "Layer-wise relevance propagation is based on a backward propagation mechanism applied sequentially to all layers of the model. Here, the model output score represents the initial relevance which is decomposed into values for each neuron of the underlying layers. The decomposition is defined by rules that are chosen for each layer, involving its weights and activations. Details on the model can be found in the [original paper](https://doi.org/10.1371/journal.pone.0130140).\n",
    "\n",
    "We could use LRP implementation of Captum but we will showcase an example using [**Zennit library**](https://zennit.readthedocs.io/en/latest/getting-started.html) that is more complete and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b9c32",
   "metadata": {},
   "source": [
    "We use `EpsilonPlus` composite, which uses `ZPlus` rule (LRP rule that only takes positive contributions) for convolutional layers and `Epsilon` rule (the most basic LRP rule) for densely connected linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.composites import EpsilonPlus\n",
    "\n",
    "\n",
    "# Create a composite instance (choice of propagation rules).\n",
    "composite = EpsilonPlus()\n",
    "xai_lrp_grad = Gradient(model, composite)\n",
    "\n",
    "# Targets as one-hot vectors for each predicted class.\n",
    "targets = torch.nn.functional.one_hot(y_pred_t, num_classes=2).float()\n",
    "\n",
    "# Make sure the input requires a gradient.\n",
    "\n",
    "with xai_lrp_grad:\n",
    "     # gradient/ relevance wrt. output/class\n",
    "     output, attr = xai_lrp_grad(X_test_t.requires_grad_(True).to(device),\n",
    "                                 targets\n",
    "                                 )\n",
    "\n",
    "attr = attr.detach().cpu().numpy()\n",
    "\n",
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3bd31",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ac9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45838797",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e6588",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_wrapper(model, inputs, targets, **kwargs):\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "    y_t = torch.nn.functional.one_hot(y_t, num_classes=2).float()\n",
    "\n",
    "\n",
    "    with xai_lrp_grad:\n",
    "     # gradient/ relevance wrt. output/class 1\n",
    "     _, attributions = xai_lrp_grad(x_t,\n",
    "                            y_t\n",
    "                            )\n",
    "     return attributions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[lrp_wrapper],\n",
    "                                            xai_names=[\"lrp\"],\n",
    "                                            x_data=X_test[:10],\n",
    "                                            y_data=pred[:10], # The target\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a00b0",
   "metadata": {},
   "source": [
    "Captum metrics do not work with Zennit LRP.\n",
    "\n",
    "You could restart this subsection with Captum LRP implementation.\n",
    "See the [documentation](https://captum.ai/api/lrp.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eae774",
   "metadata": {},
   "source": [
    ">TODO\n",
    "Compare the explanations generated by each method.\n",
    "- Which explanation method is the most faithful ?\n",
    "- Which one is the most robust ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb13be",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9a0a0",
   "metadata": {},
   "source": [
    "## Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599b2bf",
   "metadata": {},
   "source": [
    "### DiCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4c4ad",
   "metadata": {},
   "source": [
    "We now switch to counterfactual examples using the Adult dataset.\n",
    "\n",
    "For that, we will use one of the most common methods for counterfactuals generation, **DiCE** inspired from this paper: [Explaining Machine Learning Classifiers through Diverse\n",
    "Counterfactual Explanations](https://arxiv.org/pdf/1905.07697).\n",
    "\n",
    "This section is inspired from the official documentation of DiCE. Check it for more details: [DiCE package](https://interpret.ml/DiCE/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb54bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dice_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1668d1",
   "metadata": {},
   "source": [
    "Re-load the Adult dataset (here we keep it separate to keep the DiCE pipeline self-contained).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Adult from OpenML\n",
    "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "df = adult.frame.copy()\n",
    "\n",
    "# Replace '?' with NaN and drop rows with missing (simple approach)\n",
    "df = df.replace('?', np.nan).dropna()\n",
    "\n",
    "# Target is 'class': '>50K' or '<=50K' — convert to 0/1\n",
    "df['class'] = (df['class'] == '>50K').astype(int)\n",
    "\n",
    "\n",
    "# Identify categorical vs numeric columns\n",
    "target_col = 'class'\n",
    "y = df[target_col].values\n",
    "\n",
    "\n",
    "# Split into train test\n",
    "train_dataset, test_dataset, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "X_train = train_dataset.drop(columns=[target_col])\n",
    "X_test = test_dataset.drop(columns=[target_col])\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=['category','object']).columns.tolist()\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "categorical_feature_name = X_train.select_dtypes(include=['category','object']).columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline: OrdinalEncoder for categoricals, Standardize numerics\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
    "        ('cat', OrdinalEncoder(), categorical_feature_name), # We use here OrdinalEncoder to limit the number of features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full sklearn pipeline = preprocessing + RandomForest.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocess),\n",
    "                      ('classifier', RandomForestClassifier())\n",
    "                      ])\n",
    "\n",
    "# Train the model\n",
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad322208",
   "metadata": {},
   "source": [
    "We now initialize the DiCE explainer, which needs a dataset and a model. DiCE provides local explanation for the model m and requires an query input whose outcome needs to be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b68b1",
   "metadata": {},
   "source": [
    "DiCE supports *sklearn*, *tensorflow* and *pytorch* models.\n",
    "\n",
    "The variable backend below indicates the implementation type of DiCE we want to use. Four backends are supported: sklearn, TensorFlow 1.x with `backend=’TF1’`, Tensorflow 2.x with `backend=’TF2’`, and PyTorch with `backend=’PYT’`.\n",
    "\n",
    "Here, we use a trained classification model using sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28c004",
   "metadata": {},
   "source": [
    "Given the train dataset, we construct a *data object* for DiCE. Since continuous and discrete features have different ways of perturbation, we need to specify the names of the continuous features. DiCE also requires the name of the output variable that the ML model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "d = dice_ml.Data(dataframe=df,\n",
    "                continuous_features=num_cols,\n",
    "                outcome_name='class')\n",
    "\n",
    "# Using sklearn backend\n",
    "m = dice_ml.Model(model=model,\n",
    "                  backend=\"sklearn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871f79b",
   "metadata": {},
   "source": [
    "The `method` parameter specifies the explanation method. DiCE supports three methods for sklearn models:    \n",
    "    - `random` sampling,   \n",
    "    - `genetic` algorithm search: [GeCo: Quality Counterfactual Explanations in Real Time](https://arxiv.org/pdf/2101.01292)   \n",
    "    - `kd-tree` based generation: [Interpretable Counterfactual Explanations Guided by Prototypes](https://arxiv.org/pdf/1907.02584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecabdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using method=random for generating CFs\n",
    "exp = dice_ml.Dice(d, m, method=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance = X_test.iloc[0:1]\n",
    "\n",
    "# Generate counterfactuals\n",
    "e1 = exp.generate_counterfactuals(query_instance,\n",
    "                                  total_CFs=2,\n",
    "                                  desired_class=\"opposite\")\n",
    "\n",
    "# Visualization\n",
    "e1.visualize_as_dataframe(show_only_changes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0ba02",
   "metadata": {},
   "source": [
    "> **Try**:\n",
    "- Changing `query_instance` to other rows.\n",
    "- Changing the `method` use for DiCE.\n",
    "- Increasing `total_CFs`.\n",
    "- Inspecting which features change most often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21175594",
   "metadata": {},
   "source": [
    "You can try generating counterfactual explanations for other examples using the same code. It is also possible to restrict the features to vary while generating the counterfactuals with the argument `features_to_vary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing only age and education\n",
    "e2 = ...\n",
    "\n",
    "# Visualize the counterfactuals generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03df35f",
   "metadata": {},
   "source": [
    "It is also possible to specify permitted range of features within which the counterfactual should be generated with `permitted_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restricting age to be between [20,30] and Education to be either {'Doctorate', 'Prof-school'}.\n",
    "e3 = ...\n",
    "\n",
    "# Visualize the counterfactuals generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc9777",
   "metadata": {},
   "source": [
    "### Evaluations of counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c83d8",
   "metadata": {},
   "source": [
    ">TODO\n",
    "\n",
    "- Generate the counterfactuals for the first 100 samples of the Adult dataset\n",
    "- Implement one of all evaluation metrics (Validity, Proximity, Sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a20ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counterfactuals for the first 100 instances\n",
    "\n",
    "query_instances = X_test.iloc[:100]\n",
    "\n",
    "e4 = exp.generate_counterfactuals(query_instances,\n",
    "                                  total_CFs=1,\n",
    "                                  desired_class=\"opposite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the counterfactuals as a dataframe\n",
    "cf_list = [cf.final_cfs_df for cf in e4.cf_examples_list]\n",
    "cfs_all = pd.concat(cf_list, axis=0, ignore_index=True)\n",
    "\n",
    "# Retrieve original instances\n",
    "original_list = [cf.test_instance_df for cf in e4.cf_examples_list]\n",
    "original_instances = pd.concat(original_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validity(model: Pipeline,\n",
    "            counterfactuals: np.ndarray,\n",
    "            y_test: np.ndarray) -> float:\n",
    "    \"\"\"Check wheter the predictions of the counterfactuals are different.\n",
    "    We are doing the binary classification case only.\n",
    "\n",
    "    Args:\n",
    "        model (Pipeline): Sklearn model.\n",
    "        counterfactuals (np.ndarray): Counterfactuals generated.\n",
    "        y_test (np.ndarray): True labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Average validity over the whole samples.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def proximity(X_test: np.ndarray,\n",
    "              counterfactuals: np.ndarray) -> float:\n",
    "    \"\"\"Compute the proximity (distance between the original samples\n",
    "    and the generated counterfactuals).\n",
    "    To simplify consider only numerical features\n",
    "    Distance metric can be L0/L1/L2 distance.\n",
    "    Args:\n",
    "        X_test (np.ndarray): _description_\n",
    "        counterfactuals (np.ndarray): _description_\n",
    "\n",
    "    Returns:\n",
    "        float: Average proximity score.\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO\n",
    "    return 0.0\n",
    "\n",
    "def sparsity(X_test: np.ndarray,\n",
    "              counterfactuals: np.ndarray) -> float:\n",
    "    \"\"\"Compute the sparsity (number of features that have changed).\n",
    "    Args:\n",
    "        X_test (np.ndarray): _description_\n",
    "        counterfactuals (np.ndarray): _description_\n",
    "\n",
    "    Returns:\n",
    "        float: Average sparsity score.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average proximity, sparsity and validity scores\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d1390",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47300674",
   "metadata": {},
   "source": [
    "## Attacks on LIME and SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e9798",
   "metadata": {},
   "source": [
    "In this final section, we build adversarial models that:\n",
    "- behave like a biased model `f` (e.g. depending explicitly on a sensitive feature),\n",
    "- but appear innocent to explanations from LIME/SHAP by mimicking another model $\\phi$.\n",
    "\n",
    "This demonstrates that **post-hoc explanations can be fooled**.\n",
    "\n",
    "This attack was proposed by Slack et al. [Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/abs/1911.02508)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9aadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Adult from OpenML\n",
    "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "df = adult.frame.copy()\n",
    "\n",
    "# Replace '?' with NaN and drop rows with missing (simple approach)\n",
    "df = df.replace('?', np.nan).dropna()\n",
    "\n",
    "# Target is 'class': '>50K' or '<=50K' — convert to 0/1\n",
    "df['class'] = (df['class'] == '>50K').astype(int)\n",
    "\n",
    "# Add a random column -- this is what we'll have LIME/SHAP explain.\n",
    "df['unrelated_column'] = np.random.choice([0,1],size=df.shape[0])\n",
    "\n",
    "# Identify categorical vs numeric columns\n",
    "target_col = 'class'\n",
    "X_df = df.drop(columns=[target_col])\n",
    "y = df[target_col].values\n",
    "\n",
    "categorical_feature_name = X_df.select_dtypes(include=['category','object']).columns.tolist()\n",
    "categorical_feature_name += [\"unrelated_column\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93699148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline: OrdinalEncoder for categoricals, Standardize numerics\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
    "        ('cat', OrdinalEncoder(), categorical_feature_name), # We use here OrdinalEncoder to limit the number of features\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocess.fit_transform(X_df)\n",
    "feature_names_num = num_cols\n",
    "feature_names_cat = list(preprocess.named_transformers_['cat'].get_feature_names_out(categorical_feature_name))\n",
    "feature_names_all = feature_names_num + feature_names_cat\n",
    "\n",
    "categorical_feature_indcs = [feature_names_all.index(cat) for cat in feature_names_cat if cat in feature_names_all]\n",
    "\n",
    "\n",
    "# Split into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the encoded indices of specific features.\n",
    "sex_indc = feature_names_all.index('sex')\n",
    "unrelated_indcs = feature_names_all.index('unrelated_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which outcome is positive (income > 50K) or not (income < 50K).\n",
    "negative_outcome = 0\n",
    "positive_outcome = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    \"\"\" One hot encode y for binary features.  We use this to get from 1 dim ys to predict proba's.\n",
    "    This is taken from this s.o. post: https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A np.ndarray of the one hot encoded data.\n",
    "    \"\"\"\n",
    "    y_hat_one_hot = np.zeros((len(y), 2))\n",
    "    y_hat_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_hat_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa686eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sexist_model_f:\n",
    "    \"\"\"\n",
    "    Simple \"biased\" model: prediction depends solely on the encoded sex feature.\n",
    "\n",
    "    This is intentionally unrealistic but serves as a toy example of an unfair classifier.\n",
    "    \"\"\"\n",
    "    # Decision rule: classify negatively if race is woman\n",
    "    def predict(self, X):\n",
    "        return np.array([negative_outcome if x[sex_indc] == 0 else positive_outcome for x in X])\n",
    "\n",
    "    def predict_proba(self, X): \n",
    "        return one_hot_encode(self.predict(X))\n",
    "\n",
    "    def score(self, X,y):\n",
    "        return np.sum(self.predict(X)==y) / len(X)\n",
    "    \n",
    "class innocuous_model_psi:\n",
    "    \"\"\"\n",
    "    Innocuous-looking model: predicts according to the random 'unrelated_column'.\n",
    "\n",
    "    This is the model we will pretend to be for explanation methods.\n",
    "    \"\"\"\n",
    "    # Decision rule: classify according to randomly drawn column 'unrelated column'\n",
    "    def predict(self,X):\n",
    "        return np.array([negative_outcome if x[unrelated_indcs] > 0 else positive_outcome for x in X])\n",
    "\n",
    "    def predict_proba(self, X): \n",
    "        return one_hot_encode(self.predict(X))\n",
    "\n",
    "    def score(self, X,y):\n",
    "        return np.sum(self.predict(X)==y) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb28ab",
   "metadata": {},
   "source": [
    "### Fooling LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fooling_lime_shap import Adversarial_Lime_Model\n",
    "\n",
    "# Train the adversarial model for LIME with f and psi \n",
    "adv_lime = Adversarial_Lime_Model(\n",
    "    sexist_model_f(),\n",
    "    innocuous_model_psi()).train(X_train[:100],\n",
    "                                 y_train[:100],\n",
    "                                 feature_names=feature_names_all,\n",
    "                                 categorical_features=categorical_feature_indcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Let's just look at a random example in the test set\n",
    "ex_indc = np.random.choice(X_test.shape[0])\n",
    "\n",
    "# To get a baseline, we'll look at LIME applied to the biased model f\n",
    "normal_explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=adv_lime.get_column_names(),\n",
    "                                                          discretize_continuous=False,\n",
    "                                                          categorical_features=categorical_feature_indcs)\n",
    "\n",
    "normal_exp = normal_explainer.explain_instance(X_test[ex_indc], sexist_model_f().predict_proba)\n",
    "\n",
    "# Visualization of LIME explanation of the biased model.\n",
    "html = normal_exp.as_html()\n",
    "display(HTML(html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e62cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets look at the explanations on the adversarial model \n",
    "adv_explainer = lime.lime_tabular.LimeTabularExplainer(X_train,feature_names=adv_lime.get_column_names(), \n",
    "                                                       discretize_continuous=False,\n",
    "                                                       categorical_features=categorical_feature_indcs)\n",
    "\n",
    "adv_exp = adv_explainer.explain_instance(X_test[ex_indc], adv_lime.predict_proba)\n",
    "\n",
    "# Visualization of LIME explanation of the adversarial model.\n",
    "html = adv_exp.as_html()\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3f326",
   "metadata": {},
   "source": [
    "### Fooling SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f07cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fooling_lime_shap import Adversarial_Kernel_SHAP_Model\n",
    "\n",
    "\n",
    "# Train the adversarial model\n",
    "adv_shap = Adversarial_Kernel_SHAP_Model(sexist_model_f(), innocuous_model_psi()).\\\n",
    "            train(X_train[:100], y_train[:100], feature_names=feature_names_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Set the background distribution for the shap explainer using kmeans\n",
    "# The results get better if we use a lot of samples but at the expense\n",
    "# of a longer training time\n",
    "background_distribution = shap.kmeans(X_train, 100)\n",
    "\n",
    "# Let's use the shap kernel explainer and grab a point to explain\n",
    "to_examine = np.random.choice(X_test.shape[0])\n",
    "\n",
    "# Explain the biased model\n",
    "biased_kernel_explainer = shap.KernelExplainer(sexist_model_f().predict, background_distribution)\n",
    "biased_shap_values = biased_kernel_explainer.shap_values(X_test[to_examine:to_examine+1])\n",
    "\n",
    "# Explain the adversarial model\n",
    "adv_kerenel_explainer = shap.KernelExplainer(adv_shap.predict, background_distribution)\n",
    "adv_shap_values = adv_kerenel_explainer.shap_values(X_test[to_examine:to_examine+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP values for the biased model.\n",
    "shap.summary_plot(biased_shap_values,\n",
    "                  feature_names=feature_names_all,\n",
    "                  plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP values for the biased model.\n",
    "shap.summary_plot(adv_shap_values,\n",
    "                  feature_names=feature_names_all,\n",
    "                  plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e66905",
   "metadata": {},
   "source": [
    ">TODO\n",
    "\n",
    "Restart this by:\n",
    "- Building a model that is biased towards the racial attributes\n",
    "- Fooling the explanations (LIME and SHAP) not with an unrelated column but by using the educational level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691caf3",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4a147",
   "metadata": {},
   "source": [
    "## LLM explanation using Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d9bc5",
   "metadata": {},
   "source": [
    "We will just showcase one example using a gradient-based explanation (`IntegratedGradients`).\n",
    "\n",
    "You could work on more diverse examples by checking the [Captum tutorials](https://captum.ai/tutorials/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8177c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama import modeling_llama\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "# optional 4bit quantization \n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # use bfloat16 to prevent overflow in gradients\n",
    ")\n",
    "\n",
    "path = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = modeling_llama.LlamaForCausalLM.from_pretrained(path,\n",
    "                                                        device_map='cuda',\n",
    "                                                        dtype=torch.bfloat16,\n",
    "                                                        quantization_config=quantization_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"Dave lives in Palm Coast, FL and is a lawyer. His personal interests include\"\n",
    "\n",
    "target = \"playing golf, hiking, and cooking.\"\n",
    "skip_tokens = [1]  # skip the special token for the start of the text <s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cca97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids=model_input[\"input_ids\"],\n",
    "                                attention_mask=model_input[\"attention_mask\"],\n",
    "                                max_new_tokens=15\n",
    "                                )[0]\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b60d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import (\n",
    "    LayerIntegratedGradients, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69ee56",
   "metadata": {},
   "source": [
    "For LLMs, the only supported method at present is `LayerIntegratedGradients`. Layer Integrated Gradients is a variant of Integrated Gradients (see part 2) that assigns an importance score to layer inputs or outputs. \n",
    "\n",
    "To instantiate, we can simply wrap our gradient-based attribution method with `LLMGradientAttribution`. Here, we measure the importance of each input token to the embedding layer `model.embed_tokens` of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model, model.model.embed_tokens)\n",
    "\n",
    "llm_attr = LLMGradientAttribution(lig, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb734d13",
   "metadata": {},
   "source": [
    "Now that we have our LLM attribution object, we can similarly call `.attribute()` to obtain our gradient-based attributions. Right now, `LLMGradientAttribution` can only handle TextTokenInput inputs. We can visualize the attribution with respect to both the full output sequence and individual output tokens using the methods .`plot_seq_attr()` and `.plot_token_attr()`, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a006275",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTokenInput(\n",
    "    eval_prompt,\n",
    "    tokenizer,\n",
    "    skip_tokens=skip_tokens,\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\n",
    "\n",
    "attr_res.plot_seq_attr(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df371f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f934b",
   "metadata": {},
   "source": [
    "Keep in mind that the token- and sequence-wise attribution will change layer to layer. We encourage you to explore how this attribution changes with alternative layers in the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5389a6",
   "metadata": {},
   "source": [
    ">TODO\n",
    "- You can toy with this by changing the prompt and the target to see what you get.\n",
    "- Restart using `LayerGradientXActivation` or `LayerGradientShap`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20dfa3",
   "metadata": {},
   "source": [
    "## LLM explanation using LXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama import modeling_llama\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from lxt.efficient import monkey_patch\n",
    "from lxt.utils import pdf_heatmap, clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c139a4",
   "metadata": {},
   "source": [
    "We will try to explain LLM using a different paradigm: LRP (Layer-wise Relevance propagation).\n",
    "\n",
    "We use here the package [LXT](https://lxt.readthedocs.io/en/latest/index.html) that handle attention layers for LRP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74a0e5",
   "metadata": {},
   "source": [
    "To compute LRP in the backward pass, we need to modify the LLaMA module. Let’s apply the `monkey_patch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the LLaMA module to compute LRP in the backward pass\n",
    "monkey_patch(modeling_llama, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbdffaa",
   "metadata": {},
   "source": [
    "We’ll load the LLaMA model and enable gradient checkpointing to save memory.\n",
    "\n",
    "LXT also works for quantized models! However, the relevances should be accumulated in `torch.bfloat16` to prevent numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional 4bit quantization \n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # use bfloat16 to prevent overflow in gradients\n",
    ")\n",
    "\n",
    "path = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "model = modeling_llama.LlamaForCausalLM.from_pretrained(path,\n",
    "                                                        device_map='cuda',\n",
    "                                                        dtype=torch.bfloat16,\n",
    "                                                        quantization_config=quantization_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d65bbc",
   "metadata": {},
   "source": [
    "To optimize memory usage, we’ll deactivate gradients on the model parameters. Optionally, we activate gradient checkpointing, which will perform 2x forward and 1x backward passes. We set the model into `train()` mode, because right now Huggingface does not allow to activate gradient checkpointing in `eval()` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional gradient checkpointing to save memory (2x forward pass)\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# deactive gradients on parameters to save memory\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927647ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "\n",
    "prompt = \"\"\"Context: Mount Everest attracts many climbers, including highly experienced mountaineers. There are two main climbing routes, one approaching the summit from the southeast in Nepal (known as the standard route) and the other from the north in Tibet. While not posing substantial technical climbing challenges on the standard route, Everest presents dangers such as altitude sickness, weather, and wind, as well as hazards from avalanches and the Khumbu Icefall. As of November 2022, 310 people have died on Everest. Over 200 bodies remain on the mountain and have not been removed due to the dangerous conditions. The first recorded efforts to reach Everest's summit were made by British mountaineers. As Nepal did not allow foreigners to enter the country at the time, the British made several attempts on the north ridge route from the Tibetan side. After the first reconnaissance expedition by the British in 1921 reached 7,000 m (22,970 ft) on the North Col, the 1922 expedition pushed the north ridge route up to 8,320 m (27,300 ft), marking the first time a human had climbed above 8,000 m (26,247 ft). The 1924 expedition resulted in one of the greatest mysteries on Everest to this day: George Mallory and Andrew Irvine made a final summit attempt on 8 June but never returned, sparking debate as to whether they were the first to reach the top. Tenzing Norgay and Edmund Hillary made the first documented ascent of Everest in 1953, using the southeast ridge route. Norgay had reached 8,595 m (28,199 ft) the previous year as a member of the 1952 Swiss expedition. The Chinese mountaineering team of Wang Fuzhou, Gonpo, and Qu Yinhua made the first reported ascent of the peak from the north ridge on 25 May 1960. \\\n",
    "Question: How high did they climb in 1922? According to the text, the 1922 expedition reached 8,\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e2cea",
   "metadata": {},
   "source": [
    "We compute now the gradients with respect to the input embeddings. PyTorch can’t compute gradients for int64 tensors like `inputs_ids`, hence we use the bfloat16 `inputs_embeds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba04b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input embeddings\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)\n",
    "input_embeds = model.get_input_embeddings()(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cfedd",
   "metadata": {},
   "source": [
    "Make sure to activate gradient tracing for the input embeddings with `.requires_grad_()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561df340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "output_logits = model(inputs_embeds=input_embeds.requires_grad_(), use_cache=False).logits\n",
    "\n",
    "# Take the maximum logit at last token position. You can also explain any other token, or several tokens together!\n",
    "max_logits, max_indices = torch.max(output_logits[0, -1, :], dim=-1)\n",
    "\n",
    "# Backward pass (the relevance is initialized with the value of max_logits)\n",
    "max_logits.backward()\n",
    "\n",
    "# Obtain relevance. (Works at any layer in the model!)\n",
    "relevance = (input_embeds.grad * input_embeds).float().sum(-1).detach().cpu()  # Cast to float32 for higher precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab876813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize relevance between [-1, 1] for plotting\n",
    "relevance = relevance / relevance.abs().max()\n",
    "\n",
    "# remove special characters from token strings and plot the heatmap\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "tokens = clean_tokens(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aaf503",
   "metadata": {},
   "source": [
    "If you have `xelatex` installed on your destop, run the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_heatmap(tokens, relevance, path='llama_3.2_1B_instruct_heatmap.pdf', backend='xelatex') # backend='xelatex' supports more characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5505e",
   "metadata": {},
   "source": [
    "Otherwise, run this to plot to get your prompt printed inline in the notebook, with each token highlighted depending on its importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_relevance_html(tokens, relevance, max_abs=None):\n",
    "    \"\"\"\n",
    "    Display tokens inline in the notebook with background color\n",
    "    proportional to their relevance.\n",
    "\n",
    "    tokens: list[str]\n",
    "    relevance: 1D tensor/array of scores (can be positive/negative)\n",
    "    max_abs: if None, use max(|relevance|) for normalization\n",
    "    \"\"\"\n",
    "\n",
    "    # to numpy\n",
    "    if hasattr(relevance, \"detach\"):\n",
    "        relevance = relevance.detach().cpu().numpy()\n",
    "    else:\n",
    "        relevance = np.asarray(relevance)\n",
    "\n",
    "    assert len(tokens) == len(relevance), \"tokens and relevance must have same length\"\n",
    "\n",
    "    # normalization\n",
    "    if max_abs is None:\n",
    "        max_abs = np.max(np.abs(relevance)) + 1e-8\n",
    "    scores = relevance / max_abs  # now roughly in [-1, 1]\n",
    "\n",
    "    def score_to_rgba(s):\n",
    "        \"\"\"\n",
    "        Map score in [-1, 1] to a background color.\n",
    "        - negative: red\n",
    "        - positive: blue\n",
    "        - near 0: almost white\n",
    "        \"\"\"\n",
    "        s = float(np.clip(s, -1.0, 1.0))\n",
    "        alpha = abs(s)\n",
    "\n",
    "        if s > 0:\n",
    "            # blueish for positive\n",
    "            r, g, b = 100, 100, 255\n",
    "            r, g, b = 255, 0, 0\n",
    "        else:\n",
    "            # reddish for negative\n",
    "            r, g, b = 255, 100, 100\n",
    "            r, g, b = 0, 0, 255\n",
    "\n",
    "        # fade towards white as |s| -> 0\n",
    "        r = int(255 * (1 - alpha) + r * alpha)\n",
    "        g = int(255 * (1 - alpha) + g * alpha)\n",
    "        b = int(255 * (1 - alpha) + b * alpha)\n",
    "\n",
    "        return f\"rgba({r}, {g}, {b}, 0.7)\"\n",
    "\n",
    "    spans = []\n",
    "    for tok, s in zip(tokens, scores):\n",
    "        # basic HTML escaping\n",
    "        safe_tok = (\n",
    "            tok.replace(\"&\", \"&amp;\")\n",
    "               .replace(\"<\", \"&lt;\")\n",
    "               .replace(\">\", \"&gt;\")\n",
    "        )\n",
    "\n",
    "        color = score_to_rgba(s)\n",
    "        span = (\n",
    "            f\"<span style='background-color:{color}; \"\n",
    "            f\"padding:2px 3px; margin:1px; border-radius:3px; \"\n",
    "            f\"font-family:monospace;'>\"\n",
    "            f\"{safe_tok}</span>\"\n",
    "        )\n",
    "        spans.append(span)\n",
    "\n",
    "    html = \"<div style='line-height:1.8; font-size:13px;'>\" + \" \".join(spans) + \"</div>\"\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fcdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_relevance_html(tokens, relevance[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119678e0",
   "metadata": {},
   "source": [
    ">TODO:\n",
    "- Change the prompt and see what you get as the most important tokens.\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0x_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
